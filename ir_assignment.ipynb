{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "ir_assignment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdkwQ36739Wi"
      },
      "source": [
        "# Assignment 2: IR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-gYMXKq39Wj"
      },
      "source": [
        "## Preparations\n",
        "* Put all your imports, and path constants in the next cells\n",
        "* Make sure all your path constants are **relative to** ***DATA_DIR*** and **NOT hard-coded** in your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSChF98s8ErB",
        "outputId": "e7a0895f-af6b-4fc5-f754-cfd0c690882e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!pip install whoosh\n",
        "!pip install pytrec_eval\n",
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting whoosh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
            "\r\u001b[K     |▊                               | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 337kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 348kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 358kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 368kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 378kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 389kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 399kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 409kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 419kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 430kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 440kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 450kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 460kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 471kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: whoosh\n",
            "Successfully installed whoosh-2.7.4\n",
            "Collecting pytrec_eval\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/03/e6e84df6a7c1265579ab26bbe30ff7f8c22745aa77e0799bba471c0a3a19/pytrec_eval-0.5.tar.gz\n",
            "Building wheels for collected packages: pytrec-eval\n",
            "  Building wheel for pytrec-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytrec-eval: filename=pytrec_eval-0.5-cp36-cp36m-linux_x86_64.whl size=263767 sha256=5ee3d95c26b62d88fd25aa92f8daaadc90da212d2579f4fa04d3ac4d0116a047\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/66/40/1779aa0a8eb66e088669befe286f695cdfe420ba91ce662127\n",
            "Successfully built pytrec-eval\n",
            "Installing collected packages: pytrec-eval\n",
            "Successfully installed pytrec-eval-0.5\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=a3b4223ead093b054a1324817600569a73a23fe04c018d7350818cf11692c2cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIF5uBV8cRy",
        "outputId": "19a67e09-ab16-4735-f8c1-842dd7479b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import wget\n",
        "wget.download(\"https://github.com/MIE451-1513-2019/course-datasets/raw/master/government.zip\", \"government.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'government.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN-TAsbPAsRm"
      },
      "source": [
        "!unzip government.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VNv24P839Wk",
        "outputId": "a9ff2c0e-29b0-468e-9c90-9209790af714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# imports\n",
        "# Put all your imports here\n",
        "from whoosh import index, writing\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.analysis import *\n",
        "from whoosh.qparser import QueryParser\n",
        "from whoosh import qparser\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import subprocess\n",
        "import pytrec_eval\n",
        "import wget\n",
        "import nltk\n",
        "from nltk.stem import *\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CTt1NZr39Wo"
      },
      "source": [
        "DATA_DIR = \"government\"\n",
        "#\n",
        "# Put other path constants here\n",
        "#\n",
        "DOCUMENTS_DIR = os.path.join(DATA_DIR, \"documents\")\n",
        "TOPIC_FILE = os.path.join(DATA_DIR, \"gov.topics\")\n",
        "QRELS_FILE = os.path.join(DATA_DIR, \"gov.qrels\")\n",
        "DESCR_FILE = os.path.join(DATA_DIR, \"topics-with-full-descriptions.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1of-Wop39Ws"
      },
      "source": [
        "## Question 1\n",
        "Provide your text answers in the following two markdown cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_aozrL39Ws"
      },
      "source": [
        "### Q1 (a): Provide answer to Q1 (a) here [markdown cell]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFo75slGNrQ0"
      },
      "source": [
        "I think Mean Average Precision (MAP) is always a good measure for search system performance. R-prec would also be good measure for search system performance for government web site "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVMD_vZn39Wt"
      },
      "source": [
        "### Q1 (b): Provide answer to Q1 (b) here [markdown cell]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VplL0YwbQjVf"
      },
      "source": [
        "The reason why I think that Rprec is good is because it is p@R where R is the number of relevant documents for the query, it also ranks the documents. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaebEQWj39Wt"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLwEGzwz39Wu"
      },
      "source": [
        "### Q2 (a): Write your code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5ZkoByd39Wv"
      },
      "source": [
        "# Put your code for creating the index here (you can add more cells).\n",
        "# Make sure you save the final index in the variable INDEX_Q2, your query parser in QP_Q2, and your searcher in SEARCHER_Q2\n",
        "def createIndex(schema):\n",
        "    # Generate a temporary directory for the index\n",
        "    indexDir = tempfile.mkdtemp()\n",
        "\n",
        "    # create and return the index\n",
        "    return index.create_in(indexDir, schema)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8o5Upku1aRs"
      },
      "source": [
        "mySchema_Q2 = Schema(file_path = ID(stored=True),\n",
        "                  file_content = TEXT(analyzer = RegexTokenizer())) # most basic tokenizer, we need to update\n",
        "\n",
        "# now, create the index at the path INDEX_DIR based on the new schema\n",
        "myIndex_Q2 = createIndex(mySchema_Q2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2f2lvyp1cOV"
      },
      "source": [
        "def addFilesToIndex(indexObj, fileList):\n",
        "    # open writer\n",
        "    writer = writing.BufferedWriter(indexObj, period=None, limit=1000)\n",
        "\n",
        "    try:\n",
        "        # write each file to index\n",
        "        for docNum, filePath in enumerate(fileList):\n",
        "            with open(filePath, \"r\", encoding=\"utf-8\") as f:\n",
        "                fileContent = f.read()\n",
        "                writer.add_document(file_path = filePath,\n",
        "                                    file_content = fileContent)\n",
        "\n",
        "                # print status every 1000 documents\n",
        "                if (docNum+1) % 1000 == 0:\n",
        "                    print(\"already indexed:\", docNum+1)\n",
        "        print(\"done indexing.\")\n",
        "\n",
        "    finally:\n",
        "        # close the index\n",
        "        writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50yxr48c2FP9"
      },
      "source": [
        "filesToIndex = [str(filePath) for filePath in Path(DOCUMENTS_DIR).glob(\"**/*\") if filePath.is_file()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMrY3R8f50VY"
      },
      "source": [
        "filesToIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhHihWYs9Qc5",
        "outputId": "88adb74a-0939-43d4-8e34-e8ea7b50f1c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "addFilesToIndex(myIndex_Q2, filesToIndex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "already indexed: 1000\n",
            "already indexed: 2000\n",
            "already indexed: 3000\n",
            "already indexed: 4000\n",
            "done indexing.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrZ-TSHk-qhz"
      },
      "source": [
        "myQueryParser_Q2 = QueryParser(\"file_content\", schema=myIndex_Q2.schema)\n",
        "mySearcher_Q2 = myIndex_Q2.searcher()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibPPoKXF39Wx"
      },
      "source": [
        "INDEX_Q2 = myIndex_Q2 # Replace None with your index for Q2\n",
        "QP_Q2 = myQueryParser_Q2 # Replace None with your query parser for Q2\n",
        "SEARCHER_Q2 = mySearcher_Q2 # Replace None with your searcher for Q2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgcrHUB0d2JJ"
      },
      "source": [
        "def pyTrecEval(topicFile, qrelsFile, queryParser, searcher):\n",
        "    # Load topic file - a list of topics(search phrases) used for evalutation\n",
        "    with open(topicFile, \"r\") as tf:\n",
        "        topics = tf.read().splitlines()\n",
        "\n",
        "    # create an output file to which we'll write our results\n",
        "    tempOutputFile = tempfile.mkstemp()[1]\n",
        "    with open(tempOutputFile, \"w\") as outputTRECFile:\n",
        "        # for each evaluated topic:\n",
        "        # build a query and record the results in the file in TREC_EVAL format\n",
        "        for topic in topics:\n",
        "            topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
        "            #print(topic_id, topic_phrase)\n",
        "            topicQuery = queryParser.parse(topic_phrase)\n",
        "            topicResults = searcher.search(topicQuery, limit=None)\n",
        "            for (docnum, result) in enumerate(topicResults):\n",
        "                score = topicResults.score(docnum)\n",
        "                #print(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "                outputTRECFile.write(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "    with open(qrelsFile, 'r') as f_qrel:\n",
        "        qrel = pytrec_eval.parse_qrel(f_qrel)\n",
        "\n",
        "    with open(tempOutputFile, 'r') as f_run:\n",
        "        run = pytrec_eval.parse_run(f_run)\n",
        "\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
        "        qrel, pytrec_eval.supported_measures)\n",
        "\n",
        "    results = evaluator.evaluate(run)\n",
        "    def print_line(measure, scope, value):\n",
        "        print('{:25s}{:8s}{:.4f}'.format(measure, scope, value))\n",
        "\n",
        "    for query_id, query_measures in results.items():\n",
        "        # print(1)\n",
        "        # print(query_measures)\n",
        "        for measure, value in query_measures.items():\n",
        "            if measure != \"Rprec\":\n",
        "              continue\n",
        "            print_line(measure, query_id, value)\n",
        "    for measure in query_measures.keys():\n",
        "        if measure != \"Rprec\":\n",
        "              continue\n",
        "        print_line(\n",
        "            measure,\n",
        "            'all',\n",
        "            pytrec_eval.compute_aggregated_measure(\n",
        "                measure,\n",
        "                [query_measures[measure]\n",
        "                 for query_measures in results.values()]))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs-qgnks39Wz"
      },
      "source": [
        "### Q2 (b): Provide answer to Q2 (b) here [markdown cell]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ORTMvx9d5gP",
        "outputId": "767b3996-088a-4673-d51a-0a66200777ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "pyTrecEval(TOPIC_FILE, QRELS_FILE, myQueryParser_Q2, mySearcher_Q2) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rprec                    1       0.0000\n",
            "Rprec                    2       0.0000\n",
            "Rprec                    4       0.0000\n",
            "Rprec                    6       0.0000\n",
            "Rprec                    7       0.0000\n",
            "Rprec                    9       0.0000\n",
            "Rprec                    10      0.0000\n",
            "Rprec                    14      0.0000\n",
            "Rprec                    16      0.0000\n",
            "Rprec                    18      1.0000\n",
            "Rprec                    22      0.0000\n",
            "Rprec                    24      1.0000\n",
            "Rprec                    26      0.3333\n",
            "Rprec                    28      0.0000\n",
            "Rprec                    all     0.1667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TUIW6hV39Wz"
      },
      "source": [
        "### Q2 (c): Provide answer to Q2(c) here [markdown cell]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnW9kqLJTWCt"
      },
      "source": [
        "The baseline Whoosh did terriblely on my chosen measure, only query 18, 24 did well; they are 1 for 18, 24. Result for other queries is almost 0 for every other query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_e8DZ3K39W0"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks7ua8oXnuEI"
      },
      "source": [
        "def printRelName(topicFile, qrelsFile, queryParser, searcher, id):\n",
        "  with open(topicFile, \"r\") as tf:\n",
        "        topics = tf.read().splitlines()\n",
        "  for topic in topics:\n",
        "        topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
        "        if topic_id == id:\n",
        "          print(\"---------------------------Topic_id and Topic_phrase----------------------------------\")\n",
        "          print(topic_id, topic_phrase)\n",
        "          topicQuery = queryParser.parse(topic_phrase)\n",
        "          topicResults = searcher.search(topicQuery, limit=None)\n",
        "          print(\"---------------------------Return documents----------------------------------\")\n",
        "          for (docnum, result) in enumerate(topicResults):\n",
        "              score = topicResults.score(docnum)\n",
        "              print(\"%s Q0 %s %d %lf test\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "          print(\"---------------------------Relevant documents----------------------------------\")\n",
        "          with open(qrelsFile, 'r') as f_qrel:\n",
        "            qrels = f_qrel.readlines()\n",
        "            for i in qrels:\n",
        "              qid, _, doc, rel = i.rstrip().split(\" \")\n",
        "              if qid == id and rel == \"1\":\n",
        "                print(i.rstrip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRY_Z48VnvLK",
        "outputId": "f4ac4ed7-b85d-40de-faa3-526e241844d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "printRelName(TOPIC_FILE, QRELS_FILE, myQueryParser_Q2, mySearcher_Q2, \"4\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------Topic_id and Topic_phrase----------------------------------\n",
            "4 wireless communications\n",
            "---------------------------Return documents----------------------------------\n",
            "4 Q0 G00-99-2247765 0 16.449155 test\n",
            "4 Q0 G00-85-1525415 1 13.364613 test\n",
            "4 Q0 G00-05-1218739 2 12.956314 test\n",
            "4 Q0 G00-09-0774298 3 11.781349 test\n",
            "4 Q0 G00-56-4151981 4 11.367248 test\n",
            "4 Q0 G00-21-2229498 5 10.743958 test\n",
            "4 Q0 G00-98-4068688 6 10.464865 test\n",
            "4 Q0 G00-47-2117970 7 10.213356 test\n",
            "4 Q0 G00-67-0152545 8 8.392871 test\n",
            "4 Q0 G00-06-1757034 9 6.431556 test\n",
            "4 Q0 G00-78-2551063 10 3.955775 test\n",
            "4 Q0 G00-84-0274223 11 2.068438 test\n",
            "---------------------------Relevant documents----------------------------------\n",
            "4 0 G00-03-2855342 1\n",
            "4 0 G00-36-1275993 1\n",
            "4 0 G00-47-2117970 1\n",
            "4 0 G00-65-0162935 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD0gzOt239W1"
      },
      "source": [
        "### Q3 (a): Provide answer to Q3 (a) here [markdown cell]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OuOTe990XZs"
      },
      "source": [
        "The query I choose is \"wireless communications\", whose topic_id is 4. The false positive I am going to analyze is \"G00-85-1525415\" which my search engine ranked the second, but it is irrelevant by human judgement. By analzying this document, \"wireless\" term appeared many times and \"communication\" appeared 1 or 2 times, the high frequency of wireless is the reason why it is high ranked. However, the content itself was mainly focused on the transportation, which is the reason why it was judged as an irrevelant document. it is false positive because the query term appeared many times in the context of focusing on system and testing instead of taking about wireless communication. This motivates me to improve my queryparser by value more when all the query terms appeared together once than as long as one query term appeared many times.\n",
        "\n",
        "The false negative case I am going to analyze is \"G00-36-1275993\". By skimming through the document myself, I find term \"Wireless\" many times as well as the \"Communication, Telecomunnications\", which represent the same thing our query means \"wireless communications\", but they are not matched because of the tokenization. So I am going to improve our tokenizer to improve our results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzwPVMgc39W1"
      },
      "source": [
        "### Q3 (b): Write your code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAef6qiu39W2"
      },
      "source": [
        "# Put your code for creating the index here (you can add more cells).\n",
        "# Make sure you save the final index in the variable INDEX_Q3, your query parser in QP_Q3, and your searcher in SEARCHER_Q3\n",
        "from whoosh.analysis import Filter\n",
        "class CustomFilter(Filter):\n",
        "    is_morph = True\n",
        "    def __init__(self, filterFunc, *args, **kwargs):\n",
        "        self.customFilter = filterFunc\n",
        "        self.args = args\n",
        "        self.kwargs = kwargs\n",
        "    def __eq__(self):\n",
        "        return (other\n",
        "                and self.__class__ is other.__class__)\n",
        "    def __call__(self, tokens):\n",
        "        for t in tokens:\n",
        "            if t.mode == 'query': # if called by query parser\n",
        "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
        "                yield t\n",
        "            else: # == 'index' if called by indexer\n",
        "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
        "                yield t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm-jaLwl_AZi"
      },
      "source": [
        "myFilter1 = RegexTokenizer() | LowercaseFilter() | CustomFilter(WordNetLemmatizer().lemmatize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NflRKGH7_yLI"
      },
      "source": [
        "mySchema_Q3 = Schema(file_path = ID(stored=True),\n",
        "                  file_content = TEXT(analyzer = myFilter1))\n",
        "\n",
        "myIndex_Q3 = createIndex(mySchema_Q3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed5xPeVmCIUK"
      },
      "source": [
        "addFilesToIndex(myIndex_Q3, filesToIndex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE4RL9_SAE42"
      },
      "source": [
        "myQueryParser_Q3 = QueryParser(\"file_content\", schema=myIndex_Q3.schema, group=qparser.OrGroup)\n",
        "mySearcher_Q3 = myIndex_Q3.searcher()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr0LPzh6ANOF",
        "outputId": "05c04695-de42-4526-ea4b-03a8c16f4715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "pyTrecEvalMap(TOPIC_FILE, QRELS_FILE, myQueryParser_Q3, mySearcher_Q3) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "map                      1       0.0305\n",
            "map                      2       0.3000\n",
            "map                      4       0.5658\n",
            "map                      6       0.0500\n",
            "map                      7       0.1778\n",
            "map                      9       0.0385\n",
            "map                      10      0.3333\n",
            "map                      14      1.0000\n",
            "map                      16      0.1647\n",
            "map                      18      1.0000\n",
            "map                      19      0.5000\n",
            "map                      22      0.0526\n",
            "map                      24      1.0000\n",
            "map                      26      0.1081\n",
            "map                      28      0.0698\n",
            "map                      all     0.3594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H6yUwtd39W4"
      },
      "source": [
        "INDEX_Q3 = myIndex_Q3 # Replace None with your index for Q3\n",
        "QP_Q3 = myQueryParser_Q3 # Replace None with your query parser for Q3\n",
        "SEARCHER_Q3 = mySearcher_Q3 # Replace None with your searcher for Q3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD4nxtDK39W7"
      },
      "source": [
        "### Q3 (c): Provide answer to Q3 (c) here [markdown cell]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tQNZUJmUyOk"
      },
      "source": [
        "First of all, I changed the basic tokenizer to my own customerized fliter which include RegexTokenizer() | LowercaseFilter() | CustomFilter(WordNetLemmatizer().lemmatize). It improved the false negative cases where the matching words that starts with capital characters in the documents. \n",
        "Also, i changed queryparser by evaluting all the query terms appeared together more than one single query terms appeard many times, it helps with the false positive case where the one single query term appeared many times, but the document was talking something totally different from other query terms (in our case, wireless system in transporation is what the false positive document talking about, but \"wireless communications\" was what we look for)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cntL2Jdt39W8"
      },
      "source": [
        "### Q3 (d): Provide answer to Q3 (d) here [markdown cell]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlUZbe4HWNZu"
      },
      "source": [
        "it definetly improved the result! there are more non-zero Rprec value for many queries, and also the overall performance is increased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKhLuIOw39W8"
      },
      "source": [
        "### Q3 (e): Provide answer to Q3 (e) here [markdown cell]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C191N0YJWWxp"
      },
      "source": [
        "Luckily, in our case, the orignal good queries are still remaining their performance, and more other queries is getting better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl4AUf0n39W9"
      },
      "source": [
        "### Q3 (f): Provide answer to Q3 (f) here [markdown cell]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78C8Kz6TWojT"
      },
      "source": [
        "My ideas were good. first of all, the increase in performance result proves my ideas is improving the search system, and also the logic behind my improvement for false positive and false negative are reasonable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs6teq2k39W-"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaWA2olYacJH"
      },
      "source": [
        "from whoosh.qparser import MultifieldParser\n",
        "from whoosh.scoring import *\n",
        "import whoosh\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPzSoz8NX3cn"
      },
      "source": [
        "def pyTrecEvalMap(topicFile, qrelsFile, queryParser, searcher):\n",
        "    # Load topic file - a list of topics(search phrases) used for evalutation\n",
        "    with open(topicFile, \"r\") as tf:\n",
        "        topics = tf.read().splitlines()\n",
        "\n",
        "    # create an output file to which we'll write our results\n",
        "    tempOutputFile = tempfile.mkstemp()[1]\n",
        "    with open(tempOutputFile, \"w\") as outputTRECFile:\n",
        "        # for each evaluated topic:\n",
        "        # build a query and record the results in the file in TREC_EVAL format\n",
        "        for topic in topics:\n",
        "            topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
        "            # topic_phrase = topic_phrase.lower()\n",
        "            #print(topic_id, topic_phrase)\n",
        "            topicQuery = queryParser.parse(topic_phrase)\n",
        "            # C = searcher.collector(limit=10)\n",
        "            topicResults = searcher.search(topicQuery, limit=None, )\n",
        "            for (docnum, result) in enumerate(topicResults):\n",
        "                score = topicResults.score(docnum)\n",
        "                #print(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "                outputTRECFile.write(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "    with open(qrelsFile, 'r') as f_qrel:\n",
        "        qrel = pytrec_eval.parse_qrel(f_qrel)\n",
        "\n",
        "    with open(tempOutputFile, 'r') as f_run:\n",
        "        run = pytrec_eval.parse_run(f_run)\n",
        "\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
        "        qrel, pytrec_eval.supported_measures)\n",
        "\n",
        "    results = evaluator.evaluate(run)\n",
        "    def print_line(measure, scope, value):\n",
        "        print('{:25s}{:8s}{:.4f}'.format(measure, scope, value))\n",
        "\n",
        "    for query_id, query_measures in results.items():\n",
        "        # print(1)\n",
        "        # print(query_measures)\n",
        "        for measure, value in query_measures.items():\n",
        "            if measure != \"map\":\n",
        "              continue\n",
        "            print_line(measure, query_id, value)\n",
        "    for measure in query_measures.keys():\n",
        "        if measure != \"map\":\n",
        "              continue\n",
        "        print_line(\n",
        "            measure,\n",
        "            'all',\n",
        "            pytrec_eval.compute_aggregated_measure(\n",
        "                measure,\n",
        "                [query_measures[measure]\n",
        "                 for query_measures in results.values()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUx5BgqUXH_C"
      },
      "source": [
        "myFilter2 = RegexTokenizer() | LowercaseFilter() | StopFilter() | StemFilter() | IntraWordFilter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRrq41_5XJIH"
      },
      "source": [
        "mySchema_Q4 = Schema(file_path = ID(stored=True),\n",
        "                  file_content = TEXT(analyzer = myFilter2))\n",
        "\n",
        "myIndex_Q4 = createIndex(mySchema_Q4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_6S1B6DXP6y",
        "outputId": "df5d6f8d-2884-4d1c-c156-96c8bcaa1820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "addFilesToIndex(myIndex_Q4, filesToIndex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "already indexed: 1000\n",
            "already indexed: 2000\n",
            "already indexed: 3000\n",
            "already indexed: 4000\n",
            "done indexing.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwCbjc1QXTD0"
      },
      "source": [
        "myQueryParser_Q4 = QueryParser(\"file_content\", schema=myIndex_Q4.schema, group=qparser.OrGroup)\n",
        "myQueryParser_Q4.add_plugin(qparser.FuzzyTermPlugin())\n",
        "mySearcher_Q4 = myIndex_Q4.searcher()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYmCu_W1XTkD",
        "outputId": "9917ae8c-2124-44ea-b10b-e7f4e57f0a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "pyTrecEvalMap(TOPIC_FILE, QRELS_FILE, myQueryParser_Q4, mySearcher_Q4) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "map                      1       0.0433\n",
            "map                      2       0.5357\n",
            "map                      4       0.5604\n",
            "map                      6       0.0500\n",
            "map                      7       0.1778\n",
            "map                      9       0.0714\n",
            "map                      10      0.2500\n",
            "map                      14      1.0000\n",
            "map                      16      0.1601\n",
            "map                      18      1.0000\n",
            "map                      19      0.5000\n",
            "map                      22      0.0417\n",
            "map                      24      1.0000\n",
            "map                      26      0.1065\n",
            "map                      28      0.0711\n",
            "map                      all     0.3712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPjJXXa_vX_7"
      },
      "source": [
        "### Please answer the following questions here\n",
        "(a) A clear list of all final modifications made.  \n",
        "(b)  Why each modification was made – how did it help?  \n",
        "(c)  The  final  MAP  performance  that  these  modifications  attained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg7Kj_2qvWZp"
      },
      "source": [
        "- FuzzyTermPlugin, it helped a bit\n",
        "- Fliter combination from Q3, not really helped\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja1MLfC_39XG"
      },
      "source": [
        "INDEX_Q4 = myIndex_Q4 # Replace None with your index for Q4\n",
        "QP_Q4 = myQueryParser_Q4 # Replace None with your query parser for Q4\n",
        "SEARCHER_Q4 = mySearcher_Q4 # Replace None with your searcher for Q4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxIckwJc39XL"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8aamYn839XL"
      },
      "source": [
        "# Run the following cells to make sure your code returns the correct value types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jniUiH1b39XO"
      },
      "source": [
        "from whoosh.index import FileIndex\n",
        "from whoosh.qparser import QueryParser\n",
        "from whoosh.searching import Searcher\n",
        "import os.path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3YfPSiI39XR"
      },
      "source": [
        "### Q2 Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GCl6C7n39XS",
        "outputId": "9ef3c6ff-18d8-4bcb-a6da-5fc99f6bdcbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "assert(isinstance(INDEX_Q2, FileIndex)), \"Index Type\"\n",
        "assert(isinstance(QP_Q2, QueryParser)), \"Query Parser Type\"\n",
        "assert(isinstance(SEARCHER_Q2, Searcher)), \"Searcher Type\"\n",
        "print(\"Q2 Types Validated\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q2 Types Validated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9aZQ-PY39XW"
      },
      "source": [
        "### Q3 Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEKvL0g-39XX"
      },
      "source": [
        "assert(isinstance(INDEX_Q3, FileIndex)), \"Index Type\"\n",
        "assert(isinstance(QP_Q3, QueryParser)), \"Query Parser Type\"\n",
        "assert(isinstance(SEARCHER_Q3, Searcher)), \"Searcher Type\"\n",
        "print(\"Q3 Types Validated\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO5zjmDt39Xd"
      },
      "source": [
        "### Q4 Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddGGFQMP39Xd",
        "outputId": "21a578ad-296b-4a2f-b72e-21a47e9f382c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "assert(isinstance(INDEX_Q4, FileIndex)), \"Index Type\"\n",
        "assert(isinstance(QP_Q4, QueryParser)), \"Query Parser Type\"\n",
        "assert(isinstance(SEARCHER_Q4, Searcher)), \"Searcher Type\"\n",
        "print(\"Q4 Types Validated\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q4 Types Validated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGcGddmR39Xf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}